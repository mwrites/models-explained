{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10cf8a2d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "cfg = SimpleNamespace()\n",
    "cfg.device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input_shakespear.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9\n",
      "Encoded Text: tensor([2, 5, 6, 6, 7, 0, 3, 7, 8, 6, 4, 1])\n",
      "Decoded Text: Hello World!\n",
      "IMDb Vocabulary Size: 60\n",
      "Sample Encoded IMDb Text: tensor([ 6,  6, 11,  9, 11, 13,  6,  9, 16,  6,  9, 10,  1,  9,  9, 19, 11, 16,\n",
      "        19, 10, 14,  6,  6,  1,  1, 40, 51, 51, 47, 50, 19,  8,  8, 49, 34, 54,\n",
      "         7, 39, 41, 51, 40, 52, 35, 52, 50, 38, 49, 36, 46, 45])\n",
      "Sample Decoded IMDb Text: --2024-07-01 00:27:15--  https://raw.githubusercon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/_b4hnggd48d_185jzp3t9xvr0000gp/T/ipykernel_25752/4092859387.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.tokenized = torch.tensor(self.encode(text), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, text=None):\n",
    "        self.text = text\n",
    "        if text:\n",
    "            self.chars = sorted(list(set(text)))\n",
    "            self.vocab_size = len(self.chars)\n",
    "            self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
    "            self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
    "            self.tokenized = torch.tensor(self.encode(text), dtype=torch.long)\n",
    "    \n",
    "    def encode(self, s):\n",
    "            if isinstance(s, str):\n",
    "                return torch.tensor([self.stoi[c] for c in s], dtype=torch.long)\n",
    "            elif isinstance(s, list):\n",
    "                return torch.tensor([self.stoi[c] for c in s], dtype=torch.long)\n",
    "            else:\n",
    "                raise TypeError(\"Input should be a string or a list of characters\")\n",
    "            \n",
    "    def decode(self, l):\n",
    "        if isinstance(l, torch.Tensor):\n",
    "            l = l.tolist()\n",
    "        if isinstance(l, list):\n",
    "            return ''.join([self.itos[i] for i in l])\n",
    "        else:\n",
    "            raise TypeError(\"Input should be a tensor or a list of integers\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_imdb(cls):\n",
    "        # Load the IMDb dataset\n",
    "        # dataset = load_dataset(\"imdb\")\n",
    "        # texts = dataset['train']['text']\n",
    "        # combined_text = ' '.join(texts)\n",
    "\n",
    "        with open('input_shakespear.txt', 'r', encoding='utf-8') as f:\n",
    "            combined_text = f.read()\n",
    "\n",
    "        return cls(combined_text)\n",
    "\n",
    "# Example usage:\n",
    "text = \"Hello World!\"\n",
    "tokz = TextProcessor(text)\n",
    "print(\"Vocabulary Size:\", tokz.vocab_size)\n",
    "print(\"Encoded Text:\", tokz.encode(text))\n",
    "print(\"Decoded Text:\", tokz.decode(tokz.encode(text)))\n",
    "\n",
    "# Using the IMDb dataset\n",
    "tokz = TextProcessor.from_imdb()\n",
    "print(\"IMDb Vocabulary Size:\", tokz.vocab_size)\n",
    "print(\"Sample Encoded IMDb Text:\", tokz.encode(tokz.text[:50]))\n",
    "print(\"Sample Decoded IMDb Text:\", tokz.decode(tokz.encode(tokz.text[:50])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/_b4hnggd48d_185jzp3t9xvr0000gp/T/ipykernel_25752/4092859387.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.tokenized = torch.tensor(self.encode(text), dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 6,  6, 11,  ..., 10,  9,  9]),\n",
       " tensor([ 9, 24,  1,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  1,  7,  7,  7,  7,\n",
       "          7,  7,  7,  7,  7,  7,  1,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  1,\n",
       "          7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  1,  7,  7,  7,  7,  7,  7,  7,\n",
       "          7,  7,  7,  1, 18, 15,  2,  1, 15,  7, 10, 13, 26,  1,  9, 50,  0,  1,\n",
       "          1, 10,  9, 14,  9, 24,  1,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  1,\n",
       "          7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  1,  7,  7,  7,  7,  7,  7,  7,\n",
       "          7,  7,  7,  1,  7,  7,  7,  7,  7,  7,  7,  7,  7,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  1,  1,  1, 10,  9,  9,  2,  1, 14,  7, 16, 18, 26, 20,\n",
       "          9,  7, 11, 50,  0,  0, 11,  9, 11, 13,  6,  9, 16,  6,  9, 10,  1,  9,\n",
       "          9, 19, 11, 16, 19, 10, 14,  1,  3, 14,  7, 15, 15,  1, 26, 21,  8, 50,\n",
       "          4,  1,  6,  1, 58, 41, 45, 47, 52, 51,  7, 51, 55, 51, 59,  1, 50, 34,\n",
       "         53, 38, 37,  1, 32, 10, 10, 10, 14, 12, 18, 13,  8, 10, 10, 10, 14, 12,\n",
       "         18, 13, 33,  0,  0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test splits\n",
    "tokz = TextProcessor.from_imdb()\n",
    "n = int(0.9*len(tokz.tokenized)) # first 90% will be train, rest val\n",
    "train_data = tokz.tokenized[:n]\n",
    "val_data = tokz.tokenized[n:]\n",
    "\n",
    "train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([32, 8]), torch.Size([32, 8]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - cfg.block_size, (cfg.bs,))\n",
    "    x = torch.stack([data[i:i+cfg.block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+cfg.block_size+1] for i in ix])\n",
    "    x, y = x.to(cfg.device), y.to(cfg.device)\n",
    "    return x, y\n",
    "\n",
    "cfg.bs = 32\n",
    "cfg.block_size = 8\n",
    "\n",
    "len(get_batch('train')), get_batch('train')[0].shape, get_batch('train')[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arpathy/', '.... ...')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X and Y\n",
    "tokz.decode(get_batch('train')[0][0]), tokz.decode(get_batch('train')[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8])\n",
      "torch.Size([32, 8, 60])\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_sz):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_sz, vocab_sz)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        print(idx.shape)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        print(logits.shape)\n",
    "\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "model = BigramLanguageModel(tokz.vocab_size)\n",
    "# m = model.to(cfg.device)\n",
    "logits, loss = model(xb, yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "\n",
    "assert logits.shape == (cfg.bs * cfg.block_size, tokz.vocab_size), logits.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mn..-865]K0]hg[heqKi1uk-)ep6Kb8u%K16qiPiwd,PSyuidl64\n",
      "iep[’=:,52q[(b3e-]yutRs3pK\n",
      "oHv)]xOd‘e=Toe34Ta)P[Mw:COq0]Bg‘b(‘|‘q1Pdw6O7b1mSh)gg39g12nRm%t’/16o8p(q%]\n",
      "i’nnOmr9aT-kM\n",
      "‘(a6LvS5‘Lba=7.)y[c=i9T55LLcybt83lse61.BrL%KHiMwpdt%q4sbRy(q(tlL’C’98i1H=’B9tp:ei2B2o554t|agx1atu96e4B5[MbrlhT0Tc(08BrtO-vSi. gT8PK4O187b7Kb)r-%5’6u96Tgep/d Mlqo]2]\n",
      "-8\n",
      "SB2.][ L‘ktPeHKi]ea]risC/xtt5ce0(8ml‘5Klw8hmB5)3RS3psvh3‘,d4(m:Rt|tKLMdvcBxbiO‘’sx\n",
      "uqo6lHO%64//L(lqed%gna‘:Cqh0S5=B:4[sic)]31o7‘y=’7M7xlLt%vC,R.lmmC5.T(==o%bl\n",
      "8v,v\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # this is a table of size (vocab_size, vocab_size)\n",
    "        # each row corresponds to a token\n",
    "        #           token1,            token2,           token3,           ...\n",
    "        # token1 -> [p(token1|token1), p(token2|token1), p(token3|token1), ...]\n",
    "        # token2 -> [p(token1|token2), p(token2|token2), p(token3|token2), ...]\n",
    "        # token3 -> [p(token1|token3), p(token2|token3), p(token3|token3), ...]\n",
    "        # ...\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, token_id, targets=None):\n",
    "        # /!\\ these comments explain the shape during training, for inference it's a bit different\n",
    "\n",
    "        # token_numbers shape (batch_size, block_size) where block_size value is the token number from the vocab\n",
    "        #                     (32, 8)\n",
    "\n",
    "        logits = self.token_embedding_table(token_id)\n",
    "        # logits shape is (batch_size, block_size, vocab_size)\n",
    "        #                 (32, 8, 65)\n",
    "        # it means for each a batch\n",
    "        # we have a sequence of 8 positions\n",
    "        # and for each position, what is the probability of each token in the vocab\n",
    "\n",
    "        # karpathy call the logits BTC        \n",
    "        # (B, T, C) means = (batch, time, channels)\n",
    "        # B = batch size, T = position, C = channels\n",
    "        assert logits.shape[2] == tokz.vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:            \n",
    "            # batch, time, channels\n",
    "            # why do we call them time???? and channells???\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "model = BigramLanguageModel(tokz.vocab_size)\n",
    "\n",
    "# Let's look at the first pred\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=cfg.device)\n",
    "\n",
    "# We need to pass a context/sentence to autocomplete\n",
    "print(tokz.decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "# as expected we got garbage!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1092, val loss 4.1047\n",
      "step 300: train loss 1.4260, val loss 2.1074\n",
      "step 600: train loss 1.1929, val loss 2.0399\n",
      "step 900: train loss 1.1302, val loss 2.0746\n",
      "step 1200: train loss 1.1093, val loss 2.1169\n",
      "step 1500: train loss 1.1159, val loss 2.1243\n",
      "step 1800: train loss 1.1004, val loss 2.1240\n",
      "step 2100: train loss 1.1166, val loss 2.1563\n",
      "step 2400: train loss 1.0976, val loss 2.2202\n",
      "step 2700: train loss 1.0905, val loss 2.1834\n",
      "\n",
      " .0spspercom/rpaw. .................26M 43::500s\n",
      "   (1M  ...................4% 67.360cont  . .  .... 60K .......... . 50comatentecoto::8M 243::/m/coma/m husengintingt........... 0: (rchubus\n",
      " ..0s\n",
      " ..... 9% raw.... 0K   ............50s\n",
      "  8....  ............ 4 6M 9%  0s\n",
      " ........t, ............... 7..... ............ ..   .................. 1% 7............. s\n",
      "Rera/r-rconng . .....0sequs\n",
      " 00K 80K ............ ...... 0s\n",
      "\n",
      " 4|::4 .. .4, ............. 4.....................  . ....... 0K .. 41 .......\n"
     ]
    }
   ],
   "source": [
    "cfg.lr = 1e-2\n",
    "cfg.epochs = 3000\n",
    "cfg.eval_interval = 300\n",
    "cfg.eval_iters = 200 \n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, cfg, get_batch, tokz):\n",
    "        self.model = model\n",
    "        self.cfg = cfg\n",
    "        self.get_batch = get_batch\n",
    "        self.tokz = tokz\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=cfg.lr)\n",
    "        self.model = self.model.to(cfg.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self):\n",
    "        out = {}\n",
    "        self.model.eval()\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(self.cfg.eval_iters)\n",
    "            for k in range(self.cfg.eval_iters):\n",
    "                X, Y = self.get_batch(split)\n",
    "                logits, loss = self.model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "        self.model.train()\n",
    "        return out\n",
    "\n",
    "    def train(self):\n",
    "        for iter in range(self.cfg.epochs):\n",
    "            if iter % self.cfg.eval_interval == 0:\n",
    "                losses = self.estimate_loss()\n",
    "                print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "            xb, yb = self.get_batch('train')\n",
    "            logits, loss = self.model(xb, yb)\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=self.cfg.device)\n",
    "        print(self.tokz.decode(self.model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "\n",
    "trainer = Trainer(model, cfg, get_batch, tokz)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giving Context To Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Embedding (latent factors?) For Each Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()\n",
    "        # we are not doing vocab size to vocab size anymore, we are doing vocab size to n_embd\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # then we need to project it back to vocab size\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, token_id, targets=None):\n",
    "        # we are not getting logits directly but we are getting the embeddings\n",
    "        tok_emb = self.token_embedding_table(token_id) # careful (B,T,C) the C here is an embedding C\n",
    "        \n",
    "        # project it back to vocab size\n",
    "        logits = self.lm_head(tok_emb) # careful (B,T,vocab_size) the C here is the token number in the vocab size?\n",
    "\n",
    "        # at this point logits represents the probability of each token in the vocab\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:            \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since transformers do not inherently capture the order of tokens (unlike RNNs), another structure is required to capture the spatial information of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before that, let's recap what we pass to our model:\n",
    "\n",
    "# we pass a batch of token ids (sentences) for each position\n",
    "xb = get_batch('train')[0]\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8]), tensor([40, 52, 35, 52, 50, 38, 49, 36]), 'hubuserc')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and one row of token ids (sentence) looks like this\n",
    "x = get_batch('train')[0][0]\n",
    "x.shape, x, tokz.decode(x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok id is 40 torch.Size([]) \n",
      "\n",
      "each token has 32 hidden properties Embedding(60, 20)\n",
      "get the 32 hidden properties for token 40 -> tok * token embeddings -> torch.Size([20])\n",
      "\n",
      "each position has 32 hidden properties Embedding(8, 20)\n",
      "get the 32 hidden properties for position 1 to 8 -> [0, ..., 7] * pos embeddings -> torch.Size([8, 20])\n"
     ]
    }
   ],
   "source": [
    "# let's forget about the batch for now and zoom into ONE SINGLE TOKEN\n",
    "\n",
    "n_embd = 20\n",
    "\n",
    "tok = x[0]\n",
    "print('tok id is', tok.item(), tok.shape, '\\n')\n",
    "\n",
    "tok_emb = nn.Embedding(tokz.vocab_size, n_embd)\n",
    "print('each token has 32 hidden properties', tok_emb)\n",
    "print('get the 32 hidden properties for token', tok.item(), '-> tok * token embeddings ->', tok_emb(tok).shape)\n",
    "\n",
    "print('')\n",
    "pos_emb = nn.Embedding(8, n_embd)\n",
    "print('each position has 32 hidden properties', pos_emb)\n",
    "print('get the 32 hidden properties for position 1 to 8', '-> [0, ..., 7] * pos embeddings ->', pos_emb(torch.tensor([0, 1, 2, 3, 4, 5, 6, 7], device=cfg.device)).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally combine the two embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity and position info -> torch.Size([8, 20])\n"
     ]
    }
   ],
   "source": [
    "res = tok_emb(tok) + pos_emb(torch.tensor([0, 1, 2, 3, 4, 5, 6, 7], device=cfg.device))\n",
    "print('identity and position info ->', res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all of this works exactly the same at the batch level (instead of a token level) thanks to the magic of vectorization.\n",
    "Even without changing the layers shapes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each token has 32 hidden properties Embedding(60, 20)\n",
      "get the 32 hidden properties for token 40 -> tok * token embeddings -> torch.Size([32, 8, 20])\n",
      "\n",
      "each position has 32 hidden properties Embedding(8, 20)\n",
      "get the 32 hidden properties for position 1 to 8 -> [0, ..., 7] * pos embeddings -> torch.Size([8, 20])\n",
      "\n",
      "identity and position info -> torch.Size([32, 8, 20])\n"
     ]
    }
   ],
   "source": [
    "# let's forget about the batch for now and zoom into ONE SINGLE TOKEN\n",
    "print('each token has 32 hidden properties', tok_emb)\n",
    "print('get the 32 hidden properties for token', tok.item(), '-> tok * token embeddings ->', tok_emb(xb).shape)\n",
    "\n",
    "print('')\n",
    "print('each position has 32 hidden properties', pos_emb)\n",
    "print('get the 32 hidden properties for position 1 to 8', '-> [0, ..., 7] * pos embeddings ->', pos_emb(torch.arange(8, device=cfg.device)).shape)\n",
    "\n",
    "print('')\n",
    "res = tok_emb(xb) + pos_emb(torch.tensor([0, 1, 2, 3, 4, 5, 6, 7], device=cfg.device))\n",
    "print('identity and position info ->', res.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd, block_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    " \n",
    "    def forward(self, token_id, targets=None):\n",
    "        B,T = token_id.shape\n",
    "\n",
    "        # get embedding for token i\n",
    "        tok_emb = self.token_embedding_table(token_id) # careful (B,T,C) the C here is an embedding C\n",
    "\n",
    "        # we are adding the positional embeddings\n",
    "        pos_emb = self.positional_embedding_table(torch.arange(T, device=token_id.device)) # careful (T,C) the C here is an embedding C\n",
    "\n",
    "        # combine both identity and positional embeddings\n",
    "        # thanks to broadcasting, pos_emb will be added to each token in the sequence\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "        # plug that into a linear layer that will project it back to vocab size\n",
    "        logits = self.lm_head(x) # careful (B,T,vocab_size) so the C here is the token number in the vocab size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:            \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.0779, val loss 2.2225\n",
      "step 300: train loss 1.0811, val loss 2.3513\n",
      "step 600: train loss 1.0884, val loss 2.4847\n",
      "step 900: train loss 1.0913, val loss 2.4882\n",
      "step 1200: train loss 1.0845, val loss 2.5358\n",
      "step 1500: train loss 1.0815, val loss 2.5726\n",
      "step 1800: train loss 1.0730, val loss 2.6431\n",
      "step 2100: train loss 1.0957, val loss 2.6723\n",
      "step 2400: train loss 1.0968, val loss 2.6715\n",
      "step 2700: train loss 1.0831, val loss 2.7073\n",
      "\n",
      " ..........  8......... ........... ....cthuses\n",
      " ting ...gtt.................  ... ............... .. 9150congines\n",
      " 26:15.... 1% . ....... 6M ......... 2%  0cont/t.. .....4...  .. ......g s\n",
      "HTP ......c0ctintit.. ........................ ...........\n",
      "HTTTP .ctenpububus\n",
      " 54500com) [txt................... ............ ... ............. 54   0s\n",
      "   60K  .8%  ..........1525000 ....  1% 247........00K s\n",
      "Lentxtom) ......40s\n",
      " .... 8%   ...00K txthubues\n",
      " rc0K ... ..... 87. ................ ............. 2M\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, cfg, get_batch, tokz)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As humans we know that the meaning of words depends on the context. But right now this context component is not part of our model architecture.\n",
    "\n",
    "One token has no information about the position of other tokens (is that only about position though?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, What Is Context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we gonna explore a mathematical trick for an efficient implementation of the attention mechanism. Consider this one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can do right now to represent context, is to represent it as an average of previous token, ofc average is a lossy operation we lose lots of information, but it's a start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow = bag of words, average of some words\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Efficient Implementation Using Triangular Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there's a way to do that without loops, and it's to use matrix multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "b= tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]]) \n",
      "\n",
      "c= tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print('a=', a, \"\\n\")\n",
    "print('b=', b, \"\\n\")\n",
    "print('c=', c, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tril is the lower triangular part of the matrix\n",
    "torch.tril(torch.ones(3, 3))\n",
    "# so this thing looks like an accumulation of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "b= tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]]) \n",
      "\n",
      "c= tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and we can use it as an identity mask\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print('a=', a, \"\\n\")\n",
    "print('b=', b, \"\\n\")\n",
    "print('c=', c, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]]) \n",
      "\n",
      "b= tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]]) \n",
      "\n",
      "c= tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or do averages!\n",
    "\n",
    "torch.manual_seed(42)\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print('a=', a, \"\\n\")\n",
    "print('b=', b, \"\\n\")\n",
    "print('c=', c, \"\\n\")\n",
    "\n",
    "# c =\n",
    "# [first row],\n",
    "# [avg(2,6), avg(7,4)]\n",
    "# [avg(2, 6, 6), avg(7, 4, 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all of this to say that we can do that for loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 1: slow nested loop\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "like this! And ofc it's important for our model to vectorize quickly so yeah it's not essential but we will need that for a decent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: tril trick\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / torch.sum(wei, dim=1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ---> (B, T, C)\n",
    "torch.allclose(xbow, xbow2) # compare that they are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3) # compare that they are same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Problem Does Self Attention Solve? What Was People Doing Before That?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's forget bigram model for now and think about a real language model that generate words.\n",
    "\n",
    "Without attention, we can predict the probability of the next word, but this generation depends entirely on the probability of what's after a word, there's no context about the sentence or the whole paragraph/article.\n",
    "\n",
    "For example consider this example:\n",
    "__'The chicken didn't across the road because it... ?'__\n",
    "\n",
    "Without attention, without context, the model has no idea what 'it' refers to, it's just gonna generate something that usually come after it, without even considering if it's related to the chicken or the road, basically it has no idea about grammar or how sentences are constructucted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap Of Linear And Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 65])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding = index lookup\n",
    "indices = torch.tensor([1, 2, 3])\n",
    "W = nn.Parameter(torch.randn(65, 65))\n",
    "# It takes one or more indices and returns the corresponding values\n",
    "W[indices].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 65])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Layer \n",
    "input = torch.randn(32, 65)\n",
    "W = nn.Parameter(torch.randn(65, 65))\n",
    "# It takes one or more values and returns the corresponding values in that new space\n",
    "(input @ W.t()).shape #+ B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we saw how leverage the triangle trick, let's focus on how to implement self-attention.\n",
    "\n",
    "But what is self-attention?\n",
    "- It's the mechanism of baking the 'understanding' of other tokens (not only from the sentence?) into the one we are currently encoding.\n",
    "- It does so by looking at other positions in the sentence and calculating the 'attention' scores for each word PAIR in a sentence, the advatange of this method is that a word very far at the end can know its dependency to a word very far at the beginning (or anywhere actually)\n",
    "- Does it mean the embedding size of attention is vocab * vocab then?! It would be huge?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=2, out_features=16, bias=False),\n",
       " Linear(in_features=2, out_features=16, bias=False))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# version 4: self attention!\n",
    "\n",
    "# assume we have 2 tokens in the vocab size\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "x = torch.randn(C)\n",
    "\n",
    "# let's see a single Head performing self attention\n",
    "head_sz = 16\n",
    "key = nn.Linear(C, head_sz, bias=False) # why bias = False?\n",
    "query = nn.Linear(C, head_sz, bias=False)\n",
    "\n",
    "key, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16]), torch.Size([16]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basically hash the input to a key\n",
    "k = key(x) # (B,T,head_sz)\n",
    "\n",
    "# hash the input to a query\n",
    "q = query(x) # (B,T,head_sz)\n",
    "k.shape, q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it again one level higher with one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2\n",
    "x = torch.randn(T,C)\n",
    "\n",
    "# let's see a single Head performing self attention\n",
    "head_sz = 16\n",
    "key = nn.Linear(C, head_sz, bias=False) # why bias = False?\n",
    "query = nn.Linear(C, head_sz, bias=False)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "wei = q @ k.T # (T,H) @ (H,T) -> (T,T)\n",
    "wei.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again one level higher with one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2 # B = batch size, T = sequence length, C = channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head performing self attention\n",
    "head_sz = 16\n",
    "key = nn.Linear(C, head_sz, bias=False) # why bias = False?\n",
    "query = nn.Linear(C, head_sz, bias=False)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "# wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5# (B,T,H) @ (B,H,T) -> (B,T,T)\n",
    "# we need to do this version of wei otherwise we weill get super long floating values in the array somehow?! \n",
    "wei = q @ k.transpose(-2, -1)\n",
    "wei.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to plug that into our mathematical trick to represent previous tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei is not zero anymore it's the value of the dot product of the query and key (so the result of the attention?)\n",
    "# wei = torch.zeros(T, T)\n",
    "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x\n",
    "xbow3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0954,  0.0514, -0.0370,  0.1323,  0.1924, -0.0442, -0.3174, -0.5568],\n",
       "        [ 0.1274, -0.0367, -0.0224,  0.2060,  0.4789, -0.0287, -0.7192, -1.3309],\n",
       "        [-0.0565, -0.0034,  0.0150, -0.0859, -0.1710,  0.0184,  0.2639,  0.4806],\n",
       "        [ 0.1111,  0.0892, -0.0506,  0.1458,  0.1621, -0.0600, -0.2871, -0.4843],\n",
       "        [ 0.0322,  0.2394, -0.0695, -0.0173, -0.4031, -0.0789,  0.5157,  1.0506],\n",
       "        [-0.0661, -0.0053,  0.0179, -0.1001, -0.1972,  0.0219,  0.3050,  0.5549],\n",
       "        [-0.1042, -0.3516,  0.1163, -0.0621,  0.4127,  0.1334, -0.4822, -1.0401],\n",
       "        [-0.1329, -0.6591,  0.2024, -0.0203,  0.9707,  0.2307, -1.2063, -2.5023]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I am the 6th node, I am not supposed to be able to see what's oin the 7th or the 8th node, only what has come before, so we use our little masking trick with the triangular operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0954,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.1274, -0.0367,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0565, -0.0034,  0.0150,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.1111,  0.0892, -0.0506,  0.1458,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.0322,  0.2394, -0.0695, -0.0173, -0.4031,    -inf,    -inf,    -inf],\n",
       "        [-0.0661, -0.0053,  0.0179, -0.1001, -0.1972,  0.0219,    -inf,    -inf],\n",
       "        [-0.1042, -0.3516,  0.1163, -0.0621,  0.4127,  0.1334, -0.4822,    -inf],\n",
       "        [-0.1329, -0.6591,  0.2024, -0.0203,  0.9707,  0.2307, -1.2063, -2.5023]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want the negative stuff, we could do a ReLU to get rid of them, but we also want to spread the logits into probabilities, so we do a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1368, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1412, 0.1474, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1175, 0.1524, 0.1620, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1389, 0.1672, 0.1518, 0.2331, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1284, 0.1943, 0.1489, 0.1980, 0.1185, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1164, 0.1521, 0.1625, 0.1822, 0.1456, 0.2985, 0.0000, 0.0000],\n",
       "        [0.1120, 0.1076, 0.1793, 0.1893, 0.2679, 0.3337, 0.6735, 0.0000],\n",
       "        [0.1089, 0.0791, 0.1954, 0.1974, 0.4680, 0.3678, 0.3265, 1.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=1)\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can get the value of x, query, and key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = nn.Linear(C, head_sz, bias=False)\n",
    "v = value(x) # similar to key and value it's also (B,T,H)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 16]),\n",
       " tensor([[[    -0.0128,      0.0353,      0.0305,     -0.0174,      0.0394,\n",
       "                0.0103,     -0.0258,     -0.0273,      0.0612,      0.0775,\n",
       "                0.0111,     -0.0777,     -0.0626,     -0.0206,     -0.0109,\n",
       "               -0.0371],\n",
       "          [    -0.0239,      0.1240,      0.0902,     -0.0840,      0.0887,\n",
       "                0.0165,     -0.1040,     -0.0577,      0.1881,      0.2194,\n",
       "                0.0700,     -0.2267,     -0.1564,     -0.0319,     -0.0667,\n",
       "               -0.1210],\n",
       "          [    -0.0152,      0.0857,      0.0613,     -0.0594,      0.0583,\n",
       "                0.0102,     -0.0727,     -0.0376,      0.1284,      0.1484,\n",
       "                0.0503,     -0.1538,     -0.1042,     -0.0196,     -0.0478,\n",
       "               -0.0830],\n",
       "          [    -0.0475,      0.1560,      0.1275,     -0.0867,      0.1529,\n",
       "                0.0371,     -0.1197,     -0.1043,      0.2590,      0.3198,\n",
       "                0.0624,     -0.3237,     -0.2495,     -0.0739,     -0.0604,\n",
       "               -0.1601],\n",
       "          [    -0.0612,      0.1098,      0.1120,     -0.0311,      0.1730,\n",
       "                0.0518,     -0.0666,     -0.1234,      0.2177,      0.2946,\n",
       "                0.0032,     -0.2890,     -0.2591,     -0.1053,     -0.0053,\n",
       "               -0.1253],\n",
       "          [    -0.0435,     -0.0117,      0.0282,      0.0573,      0.0996,\n",
       "                0.0409,      0.0389,     -0.0772,      0.0408,      0.0941,\n",
       "               -0.0738,     -0.0796,     -0.1231,     -0.0851,      0.0677,\n",
       "               -0.0094],\n",
       "          [     0.1126,      0.1786,      0.0123,     -0.2795,     -0.2186,\n",
       "               -0.1126,     -0.2432,      0.1821,      0.0832,     -0.0524,\n",
       "                0.3165,     -0.0024,      0.2172,      0.2368,     -0.2931,\n",
       "               -0.1076],\n",
       "          [     0.4251,      0.9523,      0.2058,     -1.3013,     -0.7528,\n",
       "               -0.4378,     -1.1851,      0.6549,      0.6671,      0.1598,\n",
       "                1.4306,     -0.3988,      0.6307,      0.9254,     -1.3276,\n",
       "               -0.6530]],\n",
       " \n",
       "         [[    -0.0280,     -0.0276,      0.0066,      0.0547,      0.0589,\n",
       "                0.0273,      0.0444,     -0.0474,      0.0007,      0.0347,\n",
       "               -0.0646,     -0.0231,     -0.0656,     -0.0571,      0.0596,\n",
       "                0.0118],\n",
       "          [    -0.0202,     -0.0209,      0.0042,      0.0403,      0.0421,\n",
       "                0.0197,      0.0329,     -0.0339,     -0.0008,      0.0236,\n",
       "               -0.0473,     -0.0151,     -0.0465,     -0.0412,      0.0437,\n",
       "                0.0094],\n",
       "          [    -0.0514,     -0.0677,      0.0023,      0.1154,      0.1035,\n",
       "                0.0508,      0.0977,     -0.0848,     -0.0204,      0.0417,\n",
       "               -0.1328,     -0.0184,     -0.1086,     -0.1065,      0.1228,\n",
       "                0.0368],\n",
       "          [    -0.0686,     -0.1455,     -0.0286,      0.2027,      0.1235,\n",
       "                0.0702,      0.1833,     -0.1066,     -0.0973,     -0.0154,\n",
       "               -0.2239,      0.0530,     -0.1072,     -0.1483,      0.2077,\n",
       "                0.0981],\n",
       "          [    -0.0697,     -0.2184,     -0.0695,      0.2684,      0.1071,\n",
       "                0.0746,      0.2541,     -0.1000,     -0.1885,     -0.1063,\n",
       "               -0.2872,      0.1528,     -0.0609,     -0.1587,      0.2671,\n",
       "                0.1624],\n",
       "          [    -0.1188,     -0.4340,     -0.1538,      0.5122,      0.1665,\n",
       "                0.1300,      0.4925,     -0.1632,     -0.3997,     -0.2605,\n",
       "               -0.5420,      0.3469,     -0.0619,     -0.2776,      0.5046,\n",
       "                0.3316],\n",
       "          [    -0.0890,     -0.4432,     -0.1830,      0.4881,      0.0937,\n",
       "                0.1027,      0.4824,     -0.1082,     -0.4496,     -0.3472,\n",
       "               -0.5060,      0.4257,      0.0343,     -0.2212,      0.4719,\n",
       "                0.3533],\n",
       "          [     0.0427,     -0.8940,     -0.5469,      0.7447,     -0.3349,\n",
       "                0.0008,      0.8317,      0.1828,     -1.1906,     -1.2573,\n",
       "               -0.6945,      1.3487,      0.7381,     -0.0186,      0.6538,\n",
       "                0.8134]],\n",
       " \n",
       "         [[     0.0091,      0.0275,      0.0085,     -0.0342,     -0.0142,\n",
       "               -0.0097,     -0.0322,      0.0132,      0.0234,      0.0126,\n",
       "                0.0367,     -0.0186,      0.0086,      0.0206,     -0.0341,\n",
       "               -0.0203],\n",
       "          [     0.0296,      0.0468,      0.0032,     -0.0733,     -0.0574,\n",
       "               -0.0296,     -0.0637,      0.0478,      0.0217,     -0.0139,\n",
       "                0.0830,     -0.0004,      0.0571,      0.0622,     -0.0769,\n",
       "               -0.0281],\n",
       "          [     0.0169,      0.0305,      0.0040,     -0.0452,     -0.0318,\n",
       "               -0.0171,     -0.0401,      0.0269,      0.0172,     -0.0031,\n",
       "                0.0507,     -0.0056,      0.0301,      0.0360,     -0.0469,\n",
       "               -0.0195],\n",
       "          [     0.0943,      0.2152,      0.0480,     -0.2922,     -0.1659,\n",
       "               -0.0973,     -0.2667,      0.1448,      0.1531,      0.0406,\n",
       "                0.3207,     -0.0941,      0.1371,      0.2057,     -0.2976,\n",
       "               -0.1484],\n",
       "          [    -0.1670,     -0.5037,     -0.1552,      0.6259,      0.2617,\n",
       "                0.1778,      0.5901,     -0.2419,     -0.4268,     -0.2296,\n",
       "               -0.6718,      0.3386,     -0.1593,     -0.3781,      0.6247,\n",
       "                0.3717],\n",
       "          [     0.0006,      0.0428,      0.0241,     -0.0386,      0.0098,\n",
       "               -0.0025,     -0.0416,     -0.0040,      0.0537,      0.0536,\n",
       "                0.0372,     -0.0588,     -0.0274,      0.0060,     -0.0349,\n",
       "               -0.0378],\n",
       "          [     0.7369,      1.5743,      0.3130,     -2.1880,     -1.3247,\n",
       "               -0.7554,     -1.9809,      1.1441,      1.0593,      0.1788,\n",
       "                2.4151,     -0.5842,      1.1451,      1.5953,     -2.2404,\n",
       "               -1.0641],\n",
       "          [    -0.0919,     -1.8424,     -0.9832,      1.7289,     -0.2663,\n",
       "                0.1686,      1.8283,      0.0522,     -2.2241,     -2.1405,\n",
       "               -1.6953,      2.3829,      0.9798,     -0.3844,      1.5886,\n",
       "                1.5948]],\n",
       " \n",
       "         [[     0.0078,      0.0456,      0.0199,     -0.0488,     -0.0064,\n",
       "               -0.0093,     -0.0488,      0.0086,      0.0481,      0.0392,\n",
       "                0.0501,     -0.0469,     -0.0077,      0.0201,     -0.0467,\n",
       "               -0.0370],\n",
       "          [     0.0104,      0.0919,      0.0444,     -0.0926,     -0.0005,\n",
       "               -0.0138,     -0.0949,      0.0080,      0.1035,      0.0921,\n",
       "                0.0931,     -0.1059,     -0.0312,      0.0304,     -0.0871,\n",
       "               -0.0769],\n",
       "          [     0.0134,      0.1920,      0.0994,     -0.1843,      0.0187,\n",
       "               -0.0211,     -0.1930,      0.0015,      0.2269,      0.2135,\n",
       "                0.1823,     -0.2399,     -0.0906,      0.0474,     -0.1707,\n",
       "               -0.1645],\n",
       "          [    -0.2419,      0.3263,      0.3808,     -0.0275,      0.6559,\n",
       "                0.2099,     -0.1596,     -0.4754,      0.7237,      1.0261,\n",
       "               -0.0788,     -0.9914,     -0.9509,     -0.4287,      0.0650,\n",
       "               -0.3995],\n",
       "          [     0.0289,      0.2509,      0.1208,     -0.2531,     -0.0023,\n",
       "               -0.0381,     -0.2593,      0.0224,      0.2819,      0.2505,\n",
       "                0.2548,     -0.2884,     -0.0842,      0.0838,     -0.2382,\n",
       "               -0.2097],\n",
       "          [     0.1125,     -0.0563,     -0.1224,     -0.0717,     -0.2801,\n",
       "               -0.1020,     -0.0175,      0.2099,     -0.2153,     -0.3545,\n",
       "                0.1176,      0.3272,      0.3772,      0.2102,     -0.1062,\n",
       "                0.1010],\n",
       "          [    -0.2292,     -0.3303,     -0.0060,      0.5395,      0.4538,\n",
       "                0.2277,      0.4630,     -0.3747,     -0.1269,      0.1494,\n",
       "               -0.6161,     -0.0419,     -0.4648,     -0.4782,      0.5700,\n",
       "                0.1894],\n",
       "          [     0.5767,      1.3125,      0.2911,     -1.7835,     -1.0158,\n",
       "               -0.5948,     -1.6276,      0.8859,      0.9312,      0.2434,\n",
       "                1.9583,     -0.5700,      0.8414,      1.2576,     -1.8174,\n",
       "               -0.9042]]], grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = wei @ v # (B,T,T) @ (B,T,H) -> (B,T,H)\n",
    "out.shape, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note 1: attention as communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note 2: attention has no notion of space, operates over sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note 3: there is no communication across batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have `B,T,C = 4,8,2`, we got 4 separate pools of 8 nodes/tokens that communicate with each other. They don't communicafte accross pools (batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is just a set of vectors, they communicate, if you want them to have a notion of space (position,sequence) you need to add that component, which we will later by combining the attention with the positional embeddings???\n",
    "\n",
    "> Each example across batch dimension is of course processed completely independently and never \"talk\" to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note 4: encoder blocks vs. decoder blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The self attention block we did is called a decoder attention block because it has triangular masking, we prevented future tokens from communicating with past tokens. This setup is masking is usually used in autoregressive seettings, like language modelling.\n",
    "\n",
    "But if we want to do an encoder attention block, we just have to delete the single line that does masking `wei.masked_fill(tril == 0, float('-inf'))`, allowing all tokens to communicate with each other. For example in sentiment analysis, you would want all nodes to talk to each other because you want to get the sentiment of the whole article, not just predicting the next sequence.\n",
    "\n",
    "The beauty of attention is that it doesn't care, you can decide yourself how communication is made, from the future only, from the past only, or some crazy rule you invented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note 5: attention vs. self-attention vs. cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `self-attention` means that the keys and values are produced from the sace source as queries.\n",
    "- In `cross-attention`, the queries still get produced from `x` but the `keys` and `values` come from some other source (e.g. an encoder module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note 6: \"scaled\" self-attention. why divide by sqrt(head_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__It's an important normalization to have__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2749, grad_fn=<VarBackward0>),\n",
       " tensor(0.3859, grad_fn=<VarBackward0>),\n",
       " tensor(0.0389, grad_fn=<VarBackward0>),\n",
       " tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var(), q.var(), wei.var(), torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
