{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Decay And Saddle Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things that could help your learning algorithm is to reduce the learning rate, that's what we call weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the intuition behind why a slow learning rate is better at the end? Image your model weights as 2 points on a surface. At the beginning we are very far from our target direction, so it's easy to roughly tweak the weights to point toward north or south. But as trianing goes, the landscape becomes more complicated, more intricate, like a canyon, we can't just go north or south. We need to find the right veins, we need to take careful steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea is to decrease the learning rate over the optimizer steps. One way to do that is to exponentially reduce the learning rate."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lr(learning_rate0, epoch_num, decay_rate):\n",
    "    \"\"\"\n",
    "    Calculates updated the learning rate using exponential weight decay.\n",
    "    \n",
    "    Arguments:\n",
    "    learning_rate0 -- Original learning rate. Scalar\n",
    "    epoch_num -- Epoch number. Integer\n",
    "    decay_rate -- Decay rate. Scalar\n",
    "\n",
    "    Returns:\n",
    "    learning_rate -- Updated learning rate. Scalar \n",
    "    \"\"\"\n",
    "\n",
    "    # same as lr0^decay_rate*epoch_num\n",
    "    lr = learning_rate0 / (1 + decay_rate * epoch_num)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you set the decay to occur at every epoch, the learning rate goes to zero too quickly - even if you start with a higher learning rate. This can be fixed with fixed interval scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_lr_decay(learning_rate0, epoch_num, decay_rate, time_interval=1000):\n",
    "    \"\"\"\n",
    "    Calculates updated the learning rate using exponential weight decay.\n",
    "    \n",
    "    Arguments:\n",
    "    learning_rate0 -- Original learning rate. Scalar\n",
    "    epoch_num -- Epoch number. Integer.\n",
    "    decay_rate -- Decay rate. Scalar.\n",
    "    time_interval -- Number of epochs where you update the learning rate.\n",
    "\n",
    "    Returns:\n",
    "    learning_rate -- Updated learning rate. Scalar \n",
    "    \"\"\"\n",
    "    learning_rate = learning_rate0 / (1 + decay_rate * (np.floor(epoch_num/time_interval)))\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Truth About Local Optimums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often when we think about a model not converging, the most common analogy is that the optimizer is trapped into a hole; what we call local optimums.\n",
    "\n",
    "Thing is in a high dimensional space, these traps don't really look like holes, they look more like saddle points. Think about when would an optimizer be stuck: it would mean that the gradient is zero or very small, meaning that the optimizer doesn't know which direction; the area is ambiguous. These are saddle points.\n",
    "\n",
    "If the gradient is zero, the function is either convex light or concave light. So, the optimizer are stuck in plateaus actually, not in holes!\n",
    "\n",
    "As Andrew Ng said, because we are dealing with such high dimensional spaces, it's still hard for people to grasp what these dimensions really look like, even if dimensional reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
